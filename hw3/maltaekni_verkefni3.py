# -*- coding: utf-8 -*-
"""maltaekni_verkefni3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m3ID6cny-Oaq8-2lrwKkyFMmaYAzasgc

2. Pretrained word embeddings (30% for BS students – 25% for MS
students)

#a) Choose 6 words and find the 5 words that the models think are the most similar to these words
"""

import gensim.downloader as api

word2vec_model = api.load("word2vec-google-news-300")
glove_model = api.load("glove-wiki-gigaword-100")

words = ['run', 'eat', 'happy', 'bright', 'computer', 'music']

#most similar
def find_similar_words(model, word):
    try:
        return model.most_similar(word, topn=5)
    except KeyError:
        return f"'{word}' not in vocabulary"

for word in words:
    print(f"\nWord: {word}")
    print("Word2Vec Similar Words:", find_similar_words(word2vec_model, word))
    print("GloVe Similar Words:", find_similar_words(glove_model, word))

"""The results were mostly what I expected but some words had spome surprising outputs. For the verbs run and eat both the models gave good and similar words so that made sense.

For the adjectives Word2Vec did a better job with happy with words like glad and pleased. While GloVe gave strange results like "'m" and "i," which dont really make sense.

For bright Word2Vec had better synonyms like brighter and shining while GloVe linked it to dark and grey which are maybe a little related but not really similar.

For the nouns both models worked well for "computer", giving related words like laptop and pc, although the Word2Vec gave com_puter which I dont get.

And for music Word2Vec gave a weird result with Without_Donny_Kirshner, probably because it learned from actual names in its dataset? While GloVe focused on general words like songs and dance, which made more sense.

So overall, Word2Vec did a better job for words that have clear synonyms because it learns from context in sentences, so it understands meaning better. GloVe focuses on words that appear together in text, which sometimes gives good results like it did for computer but also brings unrelated words if they are often used in the same context.

#b) Have the models answer five analogy questions (something like king - man + woman = queen).
"""

def find_analogy(model, word_a, word_b, word_c):
    try:
        result = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=5)
        return result
    except KeyError as e:
        return f"Word not in vocabulary: {e}"

analogies = [
    ("king", "man", "woman"),  # I expect "queen"
    ("Paris", "France", "Germany"),  # "Berlin"
    ("doctor", "hospital", "school"),  #"teacher"
    ("cat", "kitten", "puppy"),  #"dog"
    ("apple", "fruit", "carrot")  # "vegetable"
]

# test each
for word_a, word_b, word_c in analogies:
    print(f"\nAnalogy: {word_a} - {word_b} + {word_c} = ?")
    print("Word2Vec Answer:", find_analogy(word2vec_model, word_a, word_b, word_c))
    print("GloVe Answer:", find_analogy(glove_model, word_a, word_b, word_c))

"""The models didnt always get the right answers. Word2Vec was sometimes close but not exact, like suggesting "teenage_girl" instead of "queen" for "king - man + woman." GloVe also struggled, giving "girl" and "person" instead of "queen."

For "Paris - France + Germany," Word2Vec gave countries instead of "Berlin," and GloVe couldnt answer because "France" was missing.

Some worked better, like "cat - kitten + puppy," where both models gave "puppies," but GloVe also had unrelated words.

I never got the right answer in the top 5, I am not sure why that is, maybe I am doing something worng? I know the models only predict based on word patterns, and not true meaning so is this even possible?

#a) Start by writing 10 sentences of your own choice (write them yourselves or get them from the news or social media etc.).

The weather today is really nice, perfect for a walk.

She loves drinking coffee in the morning to wake up.

The new movie was absolutely great, everyone is talking about it.

Scientists have discovered a new technology that could change medicine forever.

The football team celebrated their victory after a tough match.

Many people believe that artificial intelligence will shape the future.

He left his laptop at home, so he had to borrow one.

The company announced a new product that will improve their product.

She moved to a different city for a new job opportunity.

The book was so exciting that I couldnt put it down.

#b) Then take these sentences and hide one word within them, using the appropriate mask token (check if your model assumes that it is <mask>, [MASK] or

#c) Test the performance of the model. Does it manage to predict the correct word
"""

!pip install transformers torch

import torch
from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer

model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)
mlm_pipeline = pipeline("fill-mask", model=model, tokenizer=tokenizer)

sentences = [
    "[MASK] weather today is really nice, perfect for a walk.",  # the
    "She loves [MASK] coffee in the morning to wake up.",  # drinking verb
    "The new movie was absolutely [MASK], everyone is talking about it.",  # great adjective
    "Scientists have discovered a new [MASK] that could change medicine forever.",  # noun masked
    "The football team celebrated their [MASK] after a tough match.",  # noun at the end
    "Many people believe that [MASK] intelligence will shape the future.",  # First word masked
    "He [MASK] his laptop at home, so he had to borrow one.",  # verb masked
    "The company announced a new [MASK] that will improve their product.",  # noun masked
    "She moved to a different [MASK] for a new job opportunity.",  # noun at the end
    "The book was so [MASK] that I couldn’t put it down."  # adjective masked
]

for sentence in sentences:
    print(f"\nOriginal Sentence: {sentence}")
    predictions = mlm_pipeline(sentence)

    print("Top Predictions:")
    for pred in predictions:
        print(f"{pred['sequence']} (Score: {pred['score']:.4f})")

"""The model gave good predictions. It got some exactly right, like artificial for artificial intelligence and city.

Some sentences had multiple good answers, like drinking, making, and having for coffee.

A few were off, like in the last one where it was just way off, it said for example heavy instead of something like exciting.

The right word was usually in the top 5, and the model rarely gave nonsense. It sometimes picked common word associations over the best fit, which makes sense since it learns from patterns in text.

#3. Your own embeddings (30% for BS students – 25% for MS students)
"""

